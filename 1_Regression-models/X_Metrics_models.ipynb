{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1553f09-840e-4f78-be5b-b86e24774f98",
   "metadata": {},
   "source": [
    "# Evaluate Metrics of Regressor Models\n",
    "Evaluate any kind of models models (with feature eng and without it)\n",
    "\n",
    "**IMPORTANT**: The list of models to evaluate is the same, but each model could have its own feature eng, but the Input (the data_X) and the Output (the prediction) follow the same structure, so it is necesary only one notebook to evaluate the differents notebooks of training (if it is not logic for you thinking in the kaggle competitions).\n",
    "\n",
    "In this notebook, there are a parameter \"folder_models\" and in this folder are located the pkl of each model\n",
    "\n",
    "The list of Metrics to evaluate are:\n",
    "\n",
    "\n",
    "**Group 1 R2**\n",
    "- R2\n",
    "\n",
    "**Group 2 MSE**\n",
    "- MSE\n",
    "\n",
    "**Group 3 RMSE**\n",
    "- RMSE\n",
    "- RMSE MEAN RATIO\n",
    "- RMSE IQR RATIO\n",
    "\n",
    "**Group 4 MAE**\n",
    "- MAE\n",
    "- MAE MEAN RATIO\n",
    "- MAE IQR RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcc241-d92f-44a2-b96d-8ce9fc6b21de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11359038-e77d-4e60-a4c7-5ee10848acb2",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81db1f99-91ea-42ec-9977-dd25bbe874b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd80d4-872e-44e6-a874-e5a9cfb54c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fef10151-5baf-45ba-86f0-270a61e5377b",
   "metadata": {},
   "source": [
    "### 0. Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c606aeff-ea8a-489c-9ed6-3c9fe6c4ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder where the models were saved. There are the same models accepted by gurobi but the feature eng changed\n",
    "\n",
    "# list of folder with models = ['basic', 'scaler', 'poly_2', 'poly_3']\n",
    "folder_models = 'basic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090d919-3987-4927-bdcb-4f3dd81c8c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a0fb9-2ddc-49ff-8db4-c364fa56aa07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ee5ebb7-3bb4-490e-9971-c58c5baca706",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6986d7-77b6-4b7f-b7db-d9edfc62f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE LIST FEARTURES - TARGET (order data to have the same order in the features always)\n",
    "list_features = ['AveOccup', 'Latitude', 'Population', 'AveBedrms', 'HouseAge', 'Longitude', 'AveRooms', 'MedInc']\n",
    "target = 'Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b20aae-de76-4d14-98ba-5c6aae1d7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATA\n",
    "X_train = pd.read_pickle('artifacts/data/X_train.pkl')\n",
    "X_test = pd.read_pickle('artifacts/data/X_test.pkl')\n",
    "y_train = pd.read_pickle('artifacts/data/y_train.pkl')\n",
    "y_test = pd.read_pickle('artifacts/data/y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5fb8bc-3d00-400b-8dd7-a38dbed0184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape data\n",
      "\n",
      "\n",
      " TRAIN\n",
      "X_train:  (14540, 8)\n",
      "y_train:  (14540, 1)\n",
      "\n",
      "\n",
      " TEST\n",
      "X_test:  (3636, 8)\n",
      "y_test:  (3636, 1)\n"
     ]
    }
   ],
   "source": [
    "print('shape data')\n",
    "print('\\n\\n TRAIN')\n",
    "print('X_train: ', X_train.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "\n",
    "print('\\n\\n TEST')\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d92f3-9b89-4c5d-ba8a-8d842b596c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "214f0db3-646e-4258-9a14-29238415c10f",
   "metadata": {},
   "source": [
    "### 2. Load Models\n",
    "Load all the models in a dictory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345d470a-4a81-42fa-888d-165ac5978cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define list of models - list to have always the same order.\n",
    "#### In this example, the strings in the list are the same with the models were saved\n",
    "list_models_names = [\n",
    "    \"lr\",\n",
    "    \"ridge\",\n",
    "    \"lasso\",\n",
    "    \n",
    "    \"tree_simple\",\n",
    "    \"tree_default\",\n",
    "    \n",
    "    \"rf_simple\",\n",
    "    \"rf_default\",\n",
    "\n",
    "    \"gb_simple\",\n",
    "    \"gb_default\",\n",
    "\n",
    "    \"xgb_simple\",\n",
    "    \"xgb_default\",\n",
    "\n",
    "    \"mlp_simple\",\n",
    "    \"mlp_default\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4acdd849-423f-4697-b3fe-a885d1b0d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to folder models\n",
    "path_folder_models = f'artifacts/models/{folder_models}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754d019a-6005-4d78-b105-eebb4ab1a1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model: lr\n",
      "loading model: ridge\n",
      "loading model: lasso\n",
      "loading model: tree_simple\n",
      "loading model: tree_default\n",
      "loading model: rf_simple\n",
      "loading model: rf_default\n",
      "loading model: gb_simple\n",
      "loading model: gb_default\n",
      "loading model: xgb_simple\n",
      "loading model: xgb_default\n",
      "loading model: mlp_simple\n",
      "loading model: mlp_default\n"
     ]
    }
   ],
   "source": [
    "### load models\n",
    "dict_models = {}\n",
    "for model_name in list_models_names:\n",
    "    print(f'loading model: {model_name}')\n",
    "    path_model = path_folder_models + f'{model_name}.pkl'\n",
    "    with open(path_model, 'rb') as artifact:\n",
    "        dict_models[model_name] = pickle.load(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd849e-07e5-4101-b0c4-0b64ff86ca23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf94a7-d9cc-436c-a41d-7328bd9571d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6957096-823d-474e-9823-147fe156df95",
   "metadata": {},
   "source": [
    "### 3. Define Functions to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9673d1fe-e4ba-498d-9334-bd1c498d4b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.3.1\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: http://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: new BSD\n",
      "Location: /opt/anaconda3/envs/data-science-python-3-10/lib/python3.10/site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: mlflow\n"
     ]
    }
   ],
   "source": [
    "# show version scikit-learn - since version 1.4 some codes to evaluate metrics changed\n",
    "!pip show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65b3ed35-3ae4-491d-89c4-f70ba12fb677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_regressors_models(y, y_pred, model_name, decimals_round = None):\n",
    "    \"\"\"\n",
    "    Calculate a certain number of metrics to evaluate regression models. The metrics are rounded to X decimals\n",
    "\n",
    "    Args\n",
    "        y (dataframe): y true\n",
    "        y_pred (dataframe): y predicted with the model. In this codes are passed y_pred instead of X\n",
    "        model_name (string): name of the model. This name is used when the metrics are saved to identify the model of these metrics\n",
    "        decimals_round = Number of decimals to round the values. Defult None, no round the values.\n",
    "\n",
    "    Return\n",
    "        metrics_regressors (dataframe): dataframe with the metrics of the model in this datasets. Row: name metrics. Columns: value metrics\n",
    "    \"\"\"\n",
    "\n",
    "    #### R2\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    #### MSE\n",
    "    mse = mean_squared_error(y, y_pred, squared = True)\n",
    "    \n",
    "    #### RMSE\n",
    "    rmse = mean_squared_error(y, y_pred, squared = False)\n",
    "    \n",
    "    #### RMSE_MEAN_RATIO\n",
    "    # rmse mean ratio: rmse / mean_y_true\n",
    "    rmse_mean_ratio = rmse / y.mean().values[0]\n",
    "    \n",
    "    #### RMSE_IQR_RATIO\n",
    "    # rmse iqr ratio: rmse / iqr_y_true\n",
    "    rmse_iqr_ratio = rmse / iqr(y)\n",
    "    \n",
    "    #### MAE\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    \n",
    "    #### MAE_RATIO\n",
    "    mae_mean_ratio = mae / y.mean().values[0]\n",
    "    \n",
    "    #### MAE_IQR_RATIO\n",
    "    mae_iqr_ratio = mae / iqr(y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### JOIN INTO ONE DATAFRAME\n",
    "    # create dataframe\n",
    "    metrics_regressors = pd.DataFrame(index = [model_name])\n",
    "    \n",
    "    # add metrics\n",
    "    metrics_regressors['r2'] = r2\n",
    "    metrics_regressors['mse'] = mse\n",
    "    metrics_regressors['rmse'] = rmse\n",
    "    metrics_regressors['rmse_mean_ratio'] = rmse_mean_ratio\n",
    "    metrics_regressors['rmse_iqr_ratio'] = rmse_iqr_ratio\n",
    "    metrics_regressors['mae'] = mae\n",
    "    metrics_regressors['mae_mean_ratio'] = mae_mean_ratio\n",
    "    metrics_regressors['mae_iqr_ratio'] = mae_iqr_ratio\n",
    "    \n",
    "    # round\n",
    "    metrics_regressors = metrics_regressors.astype('float')\n",
    "    if decimals_round:\n",
    "        metrics_regressors = metrics_regressors.round(decimals_round)\n",
    "\n",
    "\n",
    "    return metrics_regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8786b0b5-2777-4b92-bb0f-79a33086aa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>rmse_mean_ratio</th>\n",
       "      <th>rmse_iqr_ratio</th>\n",
       "      <th>mae</th>\n",
       "      <th>mae_mean_ratio</th>\n",
       "      <th>mae_iqr_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.681</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       r2    mse   rmse  rmse_mean_ratio  rmse_iqr_ratio    mae  \\\n",
       "lr  0.681  0.398  0.631              0.3           0.443  0.471   \n",
       "\n",
       "    mae_mean_ratio  mae_iqr_ratio  \n",
       "lr           0.224          0.331  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show examples\n",
    "calculate_metrics_regressors_models(y = y_train,\n",
    "                                    y_pred = dict_models['lr'].predict(X_train),\n",
    "                                    model_name = 'lr',\n",
    "                                    decimals_round = 3\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a946575-f1bc-4f9c-b740-ae7f35127830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01c078-4cf7-46ae-aac2-a526e5417891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4d2ddbd-4ff4-4196-8cc7-e770523360fe",
   "metadata": {},
   "source": [
    "### 4. Calculate metrics train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f21acb45-59d3-4bf8-bf99-625a6465e986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating metrics: lr\n",
      "calculating metrics: ridge\n",
      "calculating metrics: lasso\n",
      "calculating metrics: tree_simple\n",
      "calculating metrics: tree_default\n",
      "calculating metrics: rf_simple\n",
      "calculating metrics: rf_default\n",
      "calculating metrics: gb_simple\n",
      "calculating metrics: gb_default\n",
      "calculating metrics: xgb_simple\n",
      "calculating metrics: xgb_default\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['AveOccup', 'Latitude', 'Longitude', 'AveBedrms', 'AveRooms', 'MedInc', 'Population', 'HouseAge'] ['AveBedrms', 'AveOccup', 'AveRooms', 'Longitude', 'Population', 'HouseAge', 'MedInc', 'Latitude']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalculating metrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# calcualte metrics\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m \u001b[43mdict_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m metrics_aux \u001b[38;5;241m=\u001b[39m calculate_metrics_regressors_models(y \u001b[38;5;241m=\u001b[39m y_train,\n\u001b[1;32m      9\u001b[0m                                                   y_pred \u001b[38;5;241m=\u001b[39m y_pred_train,\n\u001b[1;32m     10\u001b[0m                                                   model_name \u001b[38;5;241m=\u001b[39m m_name,\n\u001b[1;32m     11\u001b[0m                                                   decimals_round \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     12\u001b[0m                                                  )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# append ouput dataframe\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/data-science-python-3-10/lib/python3.10/site-packages/xgboost/sklearn.py:1168\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[1;32m   1177\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/data-science-python-3-10/lib/python3.10/site-packages/xgboost/core.py:2418\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2416\u001b[0m     data, fns, _ \u001b[38;5;241m=\u001b[39m _transform_pandas_df(data, enable_categorical)\n\u001b[1;32m   2417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[0;32m-> 2418\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_list(data) \u001b[38;5;129;01mor\u001b[39;00m _is_tuple(data):\n\u001b[1;32m   2420\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/data-science-python-3-10/lib/python3.10/site-packages/xgboost/core.py:2970\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[0;34m(self, feature_names)\u001b[0m\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m my_missing:\n\u001b[1;32m   2965\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2966\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtraining data did not have the following fields: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2967\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m my_missing)\n\u001b[1;32m   2968\u001b[0m     )\n\u001b[0;32m-> 2970\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, feature_names))\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['AveOccup', 'Latitude', 'Longitude', 'AveBedrms', 'AveRooms', 'MedInc', 'Population', 'HouseAge'] ['AveBedrms', 'AveOccup', 'AveRooms', 'Longitude', 'Population', 'HouseAge', 'MedInc', 'Latitude']"
     ]
    }
   ],
   "source": [
    "### calculate metrics for all models, TRAIN DATA\n",
    "metrics_train = pd.DataFrame()\n",
    "for m_name in list_models_names:\n",
    "    print(f'calculating metrics: {m_name}')\n",
    "\n",
    "    # calcualte metrics\n",
    "    y_pred_train = dict_models[m_name].predict(X_train)\n",
    "    metrics_aux = calculate_metrics_regressors_models(y = y_train,\n",
    "                                                      y_pred = y_pred_train,\n",
    "                                                      model_name = m_name,\n",
    "                                                      decimals_round = 3\n",
    "                                                     )\n",
    "\n",
    "    # append ouput dataframe\n",
    "    metrics_train = pd.concat([metrics_train, metrics_aux], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551caea-c68e-4939-b3c1-5b25dd9ce96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e817536-06fa-4370-80b0-c2fa52ebbf97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a122f2a0-7a6c-49eb-9e99-23a218a3eea1",
   "metadata": {},
   "source": [
    "### 5. Calculate metrics test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e74db-7512-4198-a846-4f2890669705",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate metrics for all models, TEST DATA\n",
    "metrics_test = pd.DataFrame()\n",
    "for m_name in list_models_names:\n",
    "    print(f'calculating metrics: {m_name}')\n",
    "\n",
    "    # calcualte metrics\n",
    "    y_pred_test = dict_models[m_name].predict(X_test)\n",
    "    metrics_aux = calculate_metrics_regressors_models(y = y_test,\n",
    "                                                      y_pred = y_pred_test,\n",
    "                                                      model_name = m_name,\n",
    "                                                      decimals_round = 3\n",
    "                                                     )\n",
    "\n",
    "    # append ouput dataframe\n",
    "    metrics_test = pd.concat([metrics_test, metrics_aux], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8fb58-4078-44ce-8f4c-0e09a4bf848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ce379-1986-4905-97e5-94acaed201b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9425fd34-5dd4-4ddf-8e93-73238bf8980b",
   "metadata": {},
   "source": [
    "### 6. Save Metrics\n",
    "Save metrics in a excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f7255-4058-4523-bc63-a1303e2055f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train.to_excel(f'artifacts/metrics/{folder_models}/metrics_train.xlsx')\n",
    "metrics_test.to_excel(f'artifacts/metrics/{folder_models}/metrics_test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db9f03-d92c-4e53-a991-19db20bc3b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fc771-3847-432a-850b-e160dfef9960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
